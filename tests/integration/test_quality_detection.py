"""
Integration tests for data quality anomaly detection.

Tests ensure quality operators detect intentionally injected data anomalies
in the seed data. This validates that quality checks work end-to-end.
"""

from datetime import datetime, timedelta
from unittest.mock import Mock, patch

import pytest
from airflow.exceptions import AirflowException
from airflow.models import DagBag, TaskInstance


@pytest.fixture
def dag_bag():
    """Load all DAGs from the examples directory."""
    return DagBag(dag_folder="dags/examples", include_examples=False)


@pytest.fixture
def execution_date():
    """Provide a consistent execution date for testing."""
    return datetime(2025, 1, 15, 10, 0, 0)


@pytest.fixture
def warehouse_with_quality_issues():
    """
    Mock warehouse hook with intentional data quality issues.

    Issues injected:
    - Schema: Missing column
    - Completeness: Row count below threshold
    - Freshness: Stale data
    - Uniqueness: Duplicate records
    - Null Rate: High NULL percentage
    """
    hook = Mock()

    # Schema validation - missing column
    hook.get_records = Mock(
        return_value=[
            ("customer_id", "integer", "NO", 1),
            ("customer_name", "character varying", "YES", 2),
            # Missing "email" column that should be present
        ]
    )

    # Completeness - low row count
    hook.get_first = Mock(return_value=(50,))  # Expected 1000, got 50

    return hook


class TestSchemaValidationDetection:
    """Test suite for schema validation anomaly detection."""

    def test_detects_missing_columns(self, dag_bag, execution_date):
        """Test schema validator detects missing expected columns."""
        dag_id = "demo_comprehensive_quality_v1"

        if dag_id not in dag_bag.dags:
            pytest.skip(f"{dag_id} not implemented yet")

        dag = dag_bag.get_dag(dag_id)

        # Find schema validation task
        schema_tasks = [
            task
            for task in dag.tasks
            if "schema" in task.task_id.lower() and "validat" in task.task_id.lower()
        ]

        assert len(schema_tasks) > 0, "Should have schema validation task"

        with patch("src.hooks.warehouse_hook.WarehouseHook") as mock_hook_class:
            # Mock missing column scenario
            mock_hook = Mock()
            mock_hook.get_records.return_value = [
                ("customer_id", "integer", "NO", 1),
                ("customer_name", "character varying", "YES", 2),
                # Missing "email" column
            ]
            mock_hook_class.return_value = mock_hook

            schema_task = schema_tasks[0]
            task_instance = TaskInstance(schema_task, execution_date)

            # Should detect missing column and fail or warn
            # (depending on severity configuration)
            try:
                task_instance.run(ignore_ti_state=True)
                # If task succeeds, check it logged the issue
                result = task_instance.xcom_pull(task_ids=schema_task.task_id)
                if result:
                    assert not result.get("passed", True), "Should detect missing column"
            except AirflowException:
                # Task failed - this is expected for missing columns
                pass

    def test_detects_wrong_data_types(self, dag_bag, execution_date):
        """Test schema validator detects incorrect data types."""
        dag_id = "demo_data_quality_basics_v1"

        if dag_id not in dag_bag.dags:
            pytest.skip(f"{dag_id} not implemented yet")

        dag = dag_bag.get_dag(dag_id)

        schema_tasks = [task for task in dag.tasks if "schema" in task.task_id.lower()]

        if len(schema_tasks) == 0:
            pytest.skip("No schema validation in this DAG")

    def test_detects_extra_columns(self, dag_bag, execution_date):
        """Test schema validator detects unexpected extra columns."""
        pytest.skip("Implementation depends on operator strictness configuration")


class TestCompletenessDetection:
    """Test suite for completeness check anomaly detection."""

    def test_detects_row_count_too_low(self, dag_bag, execution_date):
        """Test completeness checker detects insufficient row counts."""
        dag_id = "demo_comprehensive_quality_v1"

        if dag_id not in dag_bag.dags:
            pytest.skip(f"{dag_id} not implemented yet")

        dag = dag_bag.get_dag(dag_id)

        completeness_tasks = [
            task
            for task in dag.tasks
            if "completeness" in task.task_id.lower() or "row_count" in task.task_id.lower()
        ]

        assert len(completeness_tasks) > 0, "Should have completeness check task"

        with patch("src.hooks.warehouse_hook.WarehouseHook") as mock_hook_class:
            mock_hook = Mock()
            # Return count below minimum threshold
            mock_hook.get_first.return_value = (10,)  # Very low count
            mock_hook_class.return_value = mock_hook

            completeness_task = completeness_tasks[0]
            task_instance = TaskInstance(completeness_task, execution_date)

            # Should detect low row count
            try:
                task_instance.run(ignore_ti_state=True)
                result = task_instance.xcom_pull(task_ids=completeness_task.task_id)
                if result:
                    assert not result.get("passed", True), "Should detect low row count"
            except AirflowException:
                # Task failed - expected for critical severity
                pass

    def test_detects_row_count_too_high(self, dag_bag, execution_date):
        """Test completeness checker detects unexpectedly high row counts."""
        dag_id = "demo_comprehensive_quality_v1"

        if dag_id not in dag_bag.dags:
            pytest.skip(f"{dag_id} not implemented yet")

        dag = dag_bag.get_dag(dag_id)

        completeness_tasks = [task for task in dag.tasks if "completeness" in task.task_id.lower()]

        if len(completeness_tasks) == 0:
            pytest.skip("No completeness check in this DAG")

        with patch("src.hooks.warehouse_hook.WarehouseHook") as mock_hook_class:
            mock_hook = Mock()
            # Return count above maximum threshold
            mock_hook.get_first.return_value = (1000000,)  # Suspiciously high
            mock_hook_class.return_value = mock_hook

            completeness_task = completeness_tasks[0]
            task_instance = TaskInstance(completeness_task, execution_date)

            # Should detect anomalous row count if max threshold configured
            try:
                task_instance.run(ignore_ti_state=True)
            except AirflowException:
                pass  # May fail depending on configuration

    def test_detects_empty_tables(self, dag_bag, execution_date):
        """Test completeness checker detects completely empty tables."""
        dag_id = "demo_data_quality_basics_v1"

        if dag_id not in dag_bag.dags:
            pytest.skip(f"{dag_id} not implemented yet")

        dag_bag.get_dag(dag_id)

        with patch("src.hooks.warehouse_hook.WarehouseHook") as mock_hook_class:
            mock_hook = Mock()
            mock_hook.get_first.return_value = (0,)  # Empty table
            mock_hook_class.return_value = mock_hook

            # Should detect and handle empty table appropriately


class TestFreshnessDetection:
    """Test suite for data freshness anomaly detection."""

    def test_detects_stale_data(self, dag_bag, execution_date):
        """Test freshness checker detects data older than SLA."""
        dag_id = "demo_comprehensive_quality_v1"

        if dag_id not in dag_bag.dags:
            pytest.skip(f"{dag_id} not implemented yet")

        dag = dag_bag.get_dag(dag_id)

        freshness_tasks = [
            task
            for task in dag.tasks
            if "freshness" in task.task_id.lower() or "stale" in task.task_id.lower()
        ]

        assert len(freshness_tasks) > 0, "Should have freshness check task"

        with patch("src.hooks.warehouse_hook.WarehouseHook") as mock_hook_class:
            mock_hook = Mock()
            # Return stale timestamp (7 days old)
            stale_date = execution_date - timedelta(days=7)
            mock_hook.get_first.return_value = (stale_date,)
            mock_hook_class.return_value = mock_hook

            freshness_task = freshness_tasks[0]
            task_instance = TaskInstance(freshness_task, execution_date)

            # Should detect stale data
            try:
                task_instance.run(ignore_ti_state=True)
                result = task_instance.xcom_pull(task_ids=freshness_task.task_id)
                if result:
                    assert not result.get("passed", True), "Should detect stale data"
            except AirflowException:
                pass  # Expected for critical severity

    def test_detects_future_timestamps(self, dag_bag, execution_date):
        """Test freshness checker detects future timestamps (data quality issue)."""
        dag_id = "demo_comprehensive_quality_v1"

        if dag_id not in dag_bag.dags:
            pytest.skip(f"{dag_id} not implemented yet")

        dag_bag.get_dag(dag_id)

        with patch("src.hooks.warehouse_hook.WarehouseHook") as mock_hook_class:
            mock_hook = Mock()
            # Return future timestamp (impossible - data quality issue)
            future_date = execution_date + timedelta(days=30)
            mock_hook.get_first.return_value = (future_date,)
            mock_hook_class.return_value = mock_hook

            # Implementation may or may not detect this
            # Depends on operator logic


class TestUniquenessDetection:
    """Test suite for uniqueness check anomaly detection."""

    def test_detects_duplicate_records(self, dag_bag, execution_date):
        """Test uniqueness checker detects duplicate primary keys."""
        dag_id = "demo_comprehensive_quality_v1"

        if dag_id not in dag_bag.dags:
            pytest.skip(f"{dag_id} not implemented yet")

        dag = dag_bag.get_dag(dag_id)

        uniqueness_tasks = [
            task
            for task in dag.tasks
            if "uniqueness" in task.task_id.lower() or "duplicate" in task.task_id.lower()
        ]

        assert len(uniqueness_tasks) > 0, "Should have uniqueness check task"

        with patch("src.hooks.warehouse_hook.WarehouseHook") as mock_hook_class:
            mock_hook = Mock()
            # Return duplicate count > 0
            mock_hook.get_first.return_value = (25,)  # 25 duplicates found
            mock_hook_class.return_value = mock_hook

            uniqueness_task = uniqueness_tasks[0]
            task_instance = TaskInstance(uniqueness_task, execution_date)

            # Should detect duplicates
            try:
                task_instance.run(ignore_ti_state=True)
                result = task_instance.xcom_pull(task_ids=uniqueness_task.task_id)
                if result:
                    assert not result.get("passed", True), "Should detect duplicates"
            except AirflowException:
                pass  # Expected for strict uniqueness enforcement

    def test_detects_composite_key_duplicates(self, dag_bag, execution_date):
        """Test uniqueness checker detects duplicates in composite keys."""
        pytest.skip("Depends on DAG configuration")


class TestNullRateDetection:
    """Test suite for NULL rate anomaly detection."""

    def test_detects_high_null_percentage(self, dag_bag, execution_date):
        """Test NULL rate checker detects excessive NULL values."""
        dag_id = "demo_comprehensive_quality_v1"

        if dag_id not in dag_bag.dags:
            pytest.skip(f"{dag_id} not implemented yet")

        dag = dag_bag.get_dag(dag_id)

        null_rate_tasks = [
            task
            for task in dag.tasks
            if "null" in task.task_id.lower()
            and ("rate" in task.task_id.lower() or "check" in task.task_id.lower())
        ]

        assert len(null_rate_tasks) > 0, "Should have NULL rate check task"

        with patch("src.hooks.warehouse_hook.WarehouseHook") as mock_hook_class:
            mock_hook = Mock()
            # Return high NULL count (80% NULL rate)
            mock_hook.get_first.side_effect = [
                (800,),  # NULL count
                (1000,),  # Total count
            ]
            mock_hook_class.return_value = mock_hook

            null_rate_task = null_rate_tasks[0]
            task_instance = TaskInstance(null_rate_task, execution_date)

            # Should detect high NULL rate
            try:
                task_instance.run(ignore_ti_state=True)
                result = task_instance.xcom_pull(task_ids=null_rate_task.task_id)
                if result:
                    assert not result.get("passed", True), "Should detect high NULL rate"
            except AirflowException:
                pass  # Expected depending on threshold

    def test_detects_nulls_in_not_null_columns(self, dag_bag, execution_date):
        """Test NULL rate checker detects NULLs in NOT NULL columns."""
        pytest.skip("Depends on schema enforcement configuration")


class TestComprehensiveQualityDAG:
    """Test the comprehensive quality DAG detects all anomaly types."""

    def test_detects_all_injected_anomalies(self, dag_bag, execution_date):
        """Test comprehensive quality DAG detects all types of issues."""
        dag_id = "demo_comprehensive_quality_v1"

        if dag_id not in dag_bag.dags:
            pytest.skip(f"{dag_id} not implemented yet")

        dag = dag_bag.get_dag(dag_id)

        # Verify all 5 quality check types are present
        quality_check_types = {
            "schema": False,
            "completeness": False,
            "freshness": False,
            "uniqueness": False,
            "null": False,
        }

        for task in dag.tasks:
            task_id_lower = task.task_id.lower()
            for check_type in quality_check_types:
                if check_type in task_id_lower:
                    quality_check_types[check_type] = True

        # All 5 check types should be present
        missing_checks = [k for k, v in quality_check_types.items() if not v]
        assert len(missing_checks) == 0, f"Missing quality checks: {missing_checks}"

    def test_quality_results_logged_to_warehouse(self, dag_bag, execution_date):
        """Test quality check results are stored for audit trail."""
        pytest.skip("Requires warehouse database access")

        # Conceptual test:
        # 1. Run comprehensive quality DAG
        # 2. Query quality_check_results table
        # 3. Verify results for each check type are stored
        # 4. Verify timestamp, passed/failed status, and metrics are present

    def test_quality_failures_trigger_notifications(self, dag_bag, execution_date):
        """Test quality failures trigger appropriate notifications."""
        dag_id = "demo_comprehensive_quality_v1"

        if dag_id not in dag_bag.dags:
            pytest.skip(f"{dag_id} not implemented yet")

        dag = dag_bag.get_dag(dag_id)

        # Verify notification tasks exist for quality failures
        [
            task
            for task in dag.tasks
            if any(
                keyword in task.task_id.lower()
                for keyword in ["notify", "alert", "notification", "email", "teams", "telegram"]
            )
        ]

        # Should have at least one notification task
        # (May be configured as on_failure_callback instead of task)

    def test_quality_checks_run_before_downstream_processing(self, dag_bag):
        """Test quality checks are upstream of data processing tasks."""
        dag_id = "demo_comprehensive_quality_v1"

        if dag_id not in dag_bag.dags:
            pytest.skip(f"{dag_id} not implemented yet")

        dag = dag_bag.get_dag(dag_id)

        # Find quality check tasks
        quality_tasks = [
            task
            for task in dag.tasks
            if any(keyword in task.task_id.lower() for keyword in ["quality", "check", "validat"])
        ]

        # Find downstream processing tasks
        processing_tasks = [
            task
            for task in dag.tasks
            if any(
                keyword in task.task_id.lower()
                for keyword in ["load", "transform", "process", "aggregate"]
            )
        ]

        # Quality checks should be upstream of processing
        # (This enforces fail-fast on bad data)
        for process_task in processing_tasks:
            upstream_ids = [t.task_id for t in process_task.upstream_list]
            any(qt.task_id in upstream_ids for qt in quality_tasks)
            # Not all processing tasks need quality checks upstream
            # But at least some should
